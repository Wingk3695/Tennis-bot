# Tennis-bot
My project in 2024.

网球机器人项目。本来想做一个存档的，今天检查了一下，发现U-Net这部分语义分割的核心代码在旧电脑的另一个盘上面，似乎没有备份到百度网盘，遂放弃存档全部代码。仅在此处留一个占位符。

项目结构：

语义分割：训练了一个U-Net用来提取图像中网球场的边线和球网底部。竖线识别为红色，横线识别为绿色，球网识别为蓝色；

相机定位：预先标定相机参数，然后在语义分割结果里面应用Hough变换、形态学操作、颜色区分等各种操作，从而确定边线交点的像素坐标，进而通过仿射变换获得整个球场的俯视图，并且获得当前情况的相机位置。

上机运行：这部分工作在24年暑假完成，使用了地平线XJ3板子，在SD卡上面装了系统和各种ROS包，以及模型量化，最终可以在板子上面运行上面提到的各项功能；

硬件驱动：项目本身还购买了一个codbot底座，现在放在实验室的桌子上，底座本身可以用遥控器控制，或者用开发商提供的ros包驱动，驱动的具体方法是以一个频率持续发送某种指令，让小车移动。这部分操作让我非常困惑实现形式，最终项目也没有成功实现。

蓝牙驱动：感谢另一位项目成员研究了APP和Jbotsports发球机的通信协议，让我可以从开发板发送蓝牙指令控制发球机的一些参数。但是发球机本身能力不足，只能发半场球，力度太小，不足以实际运行。

以上是大致的项目结构。如果实验室后续有人准备了解项目，这个仓库可以用来给他们介绍。仅有的定位代码也写的乱七八糟的，应该可以从开发板的代码获取内容。

_below is summary from gemini 2.5 pro:_

# Tennis-bot: 网球场感知与定位机器人项目

## 1. 项目简介

本项目是于2024年探索的一个实验性项目，旨在开发一个能够自动感知网球场环境并进行自我定位的机器人系统（Tennis-bot）。项目核心是通过计算机视觉技术，对网球场图像进行语义分割，提取场地线条，并以此为基础解算出相机（机器人）在场地中的实时位置和姿态。

这个仓库目前作为一个项目回顾和资料占位符。由于核心的U-Net语义分割模型代码未能成功备份，仓库中的代码并不完整。但本文档详细记录了项目的技术架构、实现路径和遇到的挑战，希望能为实验室后续感兴趣的同学提供参考。

## 2. 项目技术架构

项目主要由以下几个核心模块构成：

### 2.1 语义分割 (Semantic Segmentation)
*   **模型：** 采用 **U-Net** 架构进行图像分割。
*   **功能：** 旨在从摄像头捕捉的图像中，精准识别并分割出网球场的关键元素。
*   **标签定义：**
    *   `红色`: 场地竖向边线
    *   `绿色`: 场地横向边线
    *   `蓝色`: 球网底部
*   **现状：** 核心训练与模型代码已丢失，是项目目前最主要的缺失部分。

### 2.2 相机定位 (Camera Localization)
*   **输入：** 语义分割后的图像。
*   **技术流程：**
    1.  **特征提取：** 利用 **Hough变换** 检测图像中的直线。
    2.  **线条区分：** 结合 **形态学操作** 和颜色信息（红/绿/蓝），区分横向、竖向边线和球网。
    3.  **交点计算：** 确定各边线的交点在图像中的像素坐标。
    4.  **姿态解算：** 基于预先标定的相机内参，通过 **仿射变换 (Perspective Transformation)**，将图像坐标映射到世界坐标，从而生成球场的俯视图，并计算出相机相对于球场的精确位置和姿态。
*   **代码位置：** 部分定位代码可能存在于地平线XJ3开发板的备份中，但较为混乱，需要整理。

## 3. 硬件与部署

### 3.1 上机运行
*   **硬件平台：** 地平线 **XJ3 (Sunrise X3)** 嵌入式开发板。
*   **软件环境：** 在SD卡上部署了定制的操作系统，并配置了 **ROS (Robot Operating System)** 环境。
*   **模型部署：** 对训练好的U-Net模型进行了量化，以适应嵌入式平台的算力，并成功在板子上运行了完整的“分割-定位”流程。
*   **完成时间：** 2024年暑假。

### 3.2 硬件驱动
*   **底盘：** Codbot 机器人底盘。
*   **问题与挑战：** 底盘的ROS驱动包存在设计缺陷。它要求以固定频率持续发送控制指令来维持运动，这种方式在逻辑实现上非常复杂且不稳定，导致最终未能成功驱动底盘进行自主移动。这是一个主要的遗留问题。

### 3.3 蓝牙通信
*   **目标设备：** Jbotsports 自动发球机。
*   **实现方式：** 感谢项目伙伴对APP与发球机蓝牙通信协议的逆向分析，我们成功实现了从XJ3开发板通过蓝牙发送指令，控制发球机的发射角度、力度等参数。
*   **局限性：** 发球机本身性能有限，只能发出半场球，且力度较小，无法满足实际对打或全场覆盖的需求。

## 4. 项目总结与未来展望

### 现状总结
*   **[已完成]** 核心视觉算法（分割+定位）的理论验证与嵌入式平台部署。
*   **[已完成]** 与发球机的蓝牙通信控制。
*   **[主要障碍]** 语义分割的核心模型代码丢失。
*   **[主要障碍]** 机器人底盘驱动问题未能解决，无法实现物理移动。

### 对后续工作的建议
1.  **代码恢复/重建：** 尝试从开发板中恢复代码。如果失败，可以考虑重新训练一个轻量级的语义分割模型（如使用 `PyTorch` 和 `OpenCV`）。
2.  **硬件驱动修复：** 探索替代的底盘控制方案，或从更底层研究其驱动原理，而不是依赖于官方提供的ROS包。
3.  **系统整合：** 在解决以上两个核心问题后，可以将视觉定位模块与底盘控制模块在ROS中进行整合，实现机器人的闭环自主导航。
4.  **功能扩展：** 增加网球检测与追踪功能，为实现“自动捡球”等上层应用奠定基础。
